{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization Project\n",
    "## Support Vector Machines solvers\n",
    "\n",
    "Given $m$ data points $x_i \\in \\mathbb{R}^n$ with labels $y_i \\in \\{-1,1\\}$, write a function to solve the classification problem\n",
    "\n",
    "$$ \\begin{array}l\n",
    "\\mathrm{minimize} & \\frac12 {||w||}_2^2 + C \\mathbf{1}^Tz \\\\\n",
    "\\mathrm{subject\\ to} & y_i(w^Tx_i) \\geq 1 - z_i, \\quad \\forall i \\in \\{1,\\ldots,m\\} \\\\\n",
    "& z \\succcurlyeq 0\n",
    "\\end{array} $$\n",
    "\n",
    "in the variables $w \\in \\mathbb{R}^n$, $z \\in \\mathbb{R}^m$, and its dual (warning: this problem is a bit different from the one in exercise 1).\n",
    "\n",
    "Solving this problem trains a classifier vector $w$ such that, up to some errors\n",
    "\n",
    "$$ \\begin{array}l\n",
    "w^Tx_i > 0 & \\mathrm{when}\\ y_i = 1 \\\\\n",
    "w^Tx_i < 0 & \\mathrm{when}\\ y_i = -1.\n",
    "\\end{array} $$\n",
    "\n",
    "This classifier can then be used to classify new points $x$ as positives or negatives by simply computing the scalar product $w^Tx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the barrier method to solve both primal and dual problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton(f, gradient_and_hessian, ɛ, x0, α=0.45, β=0.8):\n",
    "    \"\"\" Newton descent method.\n",
    "    @param f is the function to minimize\n",
    "    @param gradient_and_hessian is a function that returns the gradient and the hessian at x\n",
    "    @param ɛ is the required absolute precision\n",
    "    @param x0 is a strictly feasible point, i.e., f(x0) < +inf\n",
    "    @param α is a parameter for the backtracking line search\n",
    "    @param β is a parameter for the backtracking line search\n",
    "    @return an array of values\n",
    "    \"\"\"    \n",
    "    x = x0\n",
    "    values = [[copy(x), f(x)]]\n",
    "    while True:\n",
    "        # Direction computation.\n",
    "        gradient, hessian = gradient_and_hessian(x)\n",
    "        hessian_inv = inv(H)\n",
    "        dx = -hessian_inv.dot(gradient)\n",
    "        \n",
    "        # Stopping criterion.\n",
    "        λsquare = gradient.dot(-dx)\n",
    "        if λsquare/2 <= ɛ:\n",
    "            break\n",
    "        \n",
    "        # Backtracking line search.\n",
    "        t = 1.0\n",
    "        f0 = f(x)\n",
    "        Δ = -λsquare\n",
    "        while f(x + t*dx) > f0 + α*t*Δ:\n",
    "            t *= β\n",
    "        \n",
    "        # Update.\n",
    "        x += t*dx\n",
    "        values.append([copy(x), f(x)])\n",
    "    \n",
    "    return array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def barrier_method(objective, objective_gh, m, constraints, constraints_gh, ɛ, x0, t0=1.0, μ=10.0, *newton_params):\n",
    "    \"\"\" Solve the SVM classifier problem.\n",
    "    @param objective is the function to minimize\n",
    "    @param objective_gh is a function which returns the gradient and the hessian of the objective\n",
    "    @param m is the number of (inequality) constraints\n",
    "    @param constraints is a function which returns the array of the f_i(x) where f_i(x) <= 0\n",
    "    @param constraints_gh is a function which returns the array of gradients and hessians of the -log(-fi(x))\n",
    "    @param x0 is a strictly feasible point, i.e., objective(x0) < +inf and constraints(x0) < 0\n",
    "    @param t0 is a barrier parameter\n",
    "    @param μ is a barrier parameter\n",
    "    @param *newton_params are the additional newton method parameters\n",
    "    @return an array of values\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    t = t0\n",
    "    values = []\n",
    "    \n",
    "    def f(x):\n",
    "        cons = constraints(x)\n",
    "        if any(cons) <= 0:\n",
    "            return float(\"inf\")\n",
    "        return objective(x) - sum(log(cons))/t\n",
    "    def gradient_and_hessian(x):\n",
    "        obj_g, obj_h = objective_gh(x)\n",
    "        cons_g, cons_h = constraints_gh(x)\n",
    "        return obj_g + cons_g/t, obj_h + cons_h/t\n",
    "    \n",
    "    while True:\n",
    "        x = newton(f, gradient_and_hessian, ɛ, x, *newton_params)[-1][0]\n",
    "        values.append([copy(v), objective(v)])\n",
    "        # Stopping criterion.\n",
    "        if m/t <= ɛ:\n",
    "            break\n",
    "        t *= μ\n",
    "    \n",
    "    return array(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the original problem, we need to solve the following problems for different values of $t$:\n",
    "\n",
    "$$ \\begin{array}c\n",
    "\\mathrm{minimize} & \\frac12 {||w||}_2^2 + C \\mathbf{1}^Tz - \\frac1t \\left[ \\sum_{i=1}^m \\log \\left( y_i(w^Tx_i) - 1 + z_i \\right) + \\sum_{i=1}^m \\log(z_i) \\right]\n",
    "\\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can reformulate the problem to:\n",
    "\n",
    "$$ \\begin{array}c\n",
    "\\mathrm{minimize} & \\frac12{||w||}_2^2 + C\\mathbf{1}^Tz - \\frac1t \\sum_{i=1}^{2m} \\log(b_i - a_i^Tv)\n",
    "\\end{array} \\\\ \\\\\n",
    "\\mathrm{with}\\quad v = (w,z) \\in \\mathbb{R}^{n+m}\\quad b_i = \\left\\{ \\begin{array}l -1 & \\text{if}\\ i \\leq m \\\\ 0 & \\text{if}\\ i > m \\end{array}\\right. \\quad a_i = \\left\\{\\begin{array}l -(y_ix_i,e_i) & \\text{if}\\ i \\leq m \\\\ (0,e_{i-m}) & \\text{if}\\ i > m \\end{array}\\right.\\quad (e_i)_{1\\leq i\\leq m} \\text{ is the canonical base of } \\mathbb{R}^m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(w,z) = \\frac12 {||w||}_2^2 + C \\mathbf{1}^Tz - \\frac1t \\sum_{i=1}^{2m} \\log(b_i - a_i^Tv) $$\n",
    "\n",
    "$$ \\nabla_v f(v) = (w,C\\mathbf{1}) + \\frac1t\\sum_{i=1}^{2m} \\frac{a_i}{b_i - a_i^Tv} $$\n",
    "\n",
    "$$ H_f(v) = \\left( \\begin{array}c\n",
    "I_n & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{array} \\right) + \\frac1t \\sum_{i=1}^{2m} \\frac{a_i a_i^T}{\\left( b_i - a_i^Tv \\right)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_barrier(x, y, c):\n",
    "    \"\"\" Solve the SVM classifier problem.\n",
    "    @param x is an array of shape (m,n): the m points in dimension n\n",
    "    @param y is an array of shape m and of values in {-1,1}: the labels\n",
    "    @param c is the margin parameter\n",
    "    @return (w,z) where w is the classifier vector (array of shape n) and z the margin vector (array of shape m)\n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    # w = 0 and z_i = 2 is a strictly feasible point.\n",
    "    v0 = concatenate((zeros(n), 2*ones(m)))\n",
    "    a = concatenate((\n",
    "            concatenate(((y*x.T).T, eye(m)), axis=1), \n",
    "            concatenate((zeros((m, n)), eye(m)), axis=1)), \n",
    "            axis=0)\n",
    "    b = concatenate(-ones(m), zeros(m))\n",
    "    \n",
    "    scalars = array([a[i].reshape(n,1).dot(a[i].reshape(1,n)) for i in range(m)])\n",
    "    def objective(v):\n",
    "        w, z = x[:n], x[n:]\n",
    "        return sum(w**2)/2 + c*sum(z)\n",
    "    def objective_gh(v):\n",
    "        w, z = x[:n], x[n:]\n",
    "        gradient = concatenate(w, c*ones(m))\n",
    "        hessian = concatenate((\n",
    "            concatenate((eye(n), zeros((m, m))), axis=1),\n",
    "            zeros((m, n+m)),\n",
    "        ), axis=0)\n",
    "        return gradient, hessian\n",
    "    def constraints(v):\n",
    "        return a.dot(v) - b\n",
    "    def constraints_gh(v):\n",
    "        gradient = sum((a.T / div).T, axis=0)/t\n",
    "        hessian = sum((scalars.T / (div**2)).T, axis=0)/t\n",
    "        return gradient, hessian\n",
    "    \n",
    "    ɛ = 1e-10\n",
    "    values = barrier_method(objective, objective_gh, m, constraints, constraints_gh, ɛ, v0)\n",
    "    v = values[-1][0]\n",
    "    return v[:n], v[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test your code on random clouds of points (e.g. generate two classes of data points by picking two bivariate Gaussian samples with different moments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_points(n):\n",
    "    \"\"\" Generates a dataset composed of two bivariate gaussian samples with different means.\n",
    "    @param n is the number of points in each class\n",
    "    @return \"\"\"\n",
    "    σ = \n",
    "    μ1 = \n",
    "    μ2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try various values of $C > 0$ and measure out-of-sample performance (i.e. classification errors on points the algorithm has not seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot duality gap versus iteration number as well as a separation example in 2D (you may add a constant coefficient to the data points $x$ to allow classifiers that do not go through the origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Use CVX (MATLAB or OCTAVE) or CVXOPT (python), as well as LIBSVM and/or LIBLINEAR to check your results and compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Use the coordinate descent method to solve the dual. Plot duality gap versus iteration number and compare performance with the barrier method for various problem sizes (vary the number of samples and record to total CPU time required by each code to reduce the gap by a factor ${10}^{-3}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Use the logarithmic barrier code you wrote in HW1 to solve a small random instance of the primal problem using the ACCPM algorithm. Plot an upper bound on the distance to optimality in semilog scale and try various constraint dropping strategies. Compare convergence with the two other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project: Convex Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier Problem\n",
    "\n",
    "For $x_1,  x_2, \\ldots, x_m \\in \\mathbb{R}^n$, $y_1,\\ldots,y_m \\in \\mathbb{R}$, we consider the following minimization problem:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{w, z}{\\text{minimize}}\n",
    "& & \\frac12 \\|w\\|_2^2 + C \\mathbf{1}^Tz \\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i) \\ge 1 - z_i, \\; i = 1, \\ldots, m \\\\\n",
    "& & & z \\ge 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "If we compute the dual, we get the following minimization problem:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\alpha}{\\text{minimize}}\n",
    "& & \\frac12 \\alpha^T \\mathbf{diag}(y)XX^T\\textbf{diag}(y)\\alpha - \\mathbf{1}^T\\alpha \\\\\n",
    "& \\text{subject to}\n",
    "& & 0 \\le \\alpha \\le C\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where we can recover the primal solution: $w= \\sum_{i=1}^m \\alpha_i y_i x_i$\n",
    "\n",
    "Thus, we use the barrier method to solve both primal and duals:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\alpha}{\\text{minimize}}\n",
    "& & \\frac12 \\alpha^T \\mathbf{diag}(y)XX^T\\textbf{diag}(y)\\alpha - \\mathbf{1}^T\\alpha - \\frac1t \\sum_{i=1}^m (\\log(\\alpha_i) + \\log(C - \\alpha_i))\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We first compute the formula for the gradient and the hessian matrix.\n",
    "We first set $H = \\mathbf{diag}(y)XX^T\\textbf{diag}(y)$.\n",
    "Then, we get:\n",
    "$$\\nabla f = H \\alpha - \\mathbf{1} + \\frac1t \\sum_{i=1}^m (-\\frac1{\\alpha_i} + \\frac1{C-\\alpha_i})e_i\\\\\n",
    "\\nabla^2 f = H + \\frac1t \\textbf{diag}(\\frac1{\\alpha_i^2} + \\frac1{(C-\\alpha_i)^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newton_svm(H, C, epsilon, alpha, beta, t=1., x_start=None, display=True, optimal_value=None):\n",
    "    n, _ = H.shape\n",
    "    inv_t = 1 / t\n",
    "    \n",
    "    # We arbitrarily choose C/2 as first position, if no x_start is supplied.\n",
    "    # Otherwise, we assume that this first x is strictly feasible.\n",
    "    if x_start is None:\n",
    "        x = np.ones(n) * C / 2\n",
    "    else:\n",
    "        x = x_start\n",
    "        \n",
    "    errs = []\n",
    "    neg_ones = -np.ones(n)\n",
    "    inds = np.arange(0, n)\n",
    "    Hx = H.dot(x)\n",
    "    fx = 0.5 * x.dot(Hx) + neg_ones.dot(x) - inv_t * np.sum(np.log(x) + np.log(C - x))\n",
    "\n",
    "    error = False\n",
    "    while True:\n",
    "        # We compute the gradient and the hessian at current point x.\n",
    "        gradient = Hx + neg_ones + (1 / (C - x) - 1 / x) * inv_t\n",
    "#         hessian = np.copy(H)\n",
    "        modif = inv_t * (1 / (x * x) + 1 / (C - x) ** 2)\n",
    "        H[inds, inds] += modif\n",
    "        direction = -linalg.inv(H).dot(gradient)\n",
    "        decrement = -gradient.T.dot(direction)\n",
    "        H[inds, inds] -= modif\n",
    "        \n",
    "        step_size = 1\n",
    "        prev_fx = np.nan\n",
    "        # We perform an alpha beta line search in the direction given by Newton's method.\n",
    "        while True:\n",
    "            new_x = x + step_size * direction\n",
    "            new_Hx = H.dot(new_x)\n",
    "            new_fx = 0.5 * new_x.dot(new_Hx) + neg_ones.dot(new_x) - inv_t * np.sum(np.log(new_x) + np.log(C - new_x))\n",
    "            \n",
    "            # If new_fx is 'nan', the following equality returns False.\n",
    "            # Thus, we continue to reduce the step size at least until new_x becomes feasible.\n",
    "            if new_fx <= fx - decrement * step_size * alpha or prev_fx < new_fx:\n",
    "                break\n",
    "\n",
    "            step_size *= beta\n",
    "            prev_fx = new_fx\n",
    "            \n",
    "        x = new_x\n",
    "        fx = new_fx\n",
    "        Hx = new_Hx\n",
    "        \n",
    "        # We use the Newton's decrement as an approximation of the error, if no optimal value is given to the function.\n",
    "        err = decrement / 2 if optimal_value is None else fx - optimal_value\n",
    "        errs.append(err)\n",
    "        \n",
    "        if err < epsilon:\n",
    "            break\n",
    "    \n",
    "    if errs[-1] <= 0.:\n",
    "        errs[-1] = epsilon\n",
    "\n",
    "    if display:\n",
    "        semilogy()\n",
    "        plot(arange(1, len(errs) + 1), errs)\n",
    "        show()\n",
    "    return x, fx\n",
    "\n",
    "\n",
    "def svm(X, y, C, kernel=None, precision=None, alpha=0.5, beta=0.5, gamma=10, show_step_details=False, t=1., show_steps=False):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    if kernel is None:\n",
    "        def linear_kernel(X1, X2):\n",
    "            return X1.dot(X2.T)\n",
    "        def affine_kernel(X1, X2):\n",
    "            return X1.dot(X2.T) + 1\n",
    "        # We use linear kernel (with an offset).\n",
    "        kernel = affine_kernel\n",
    "    H = kernel(X, X) * y[:, None] * y[None, :]\n",
    "\n",
    "    α = None\n",
    "    if precision is None:\n",
    "        precision = 1e-13 * C\n",
    "    elif precision < 1e-13 * C:\n",
    "        print(\"Modified precision to %.0e to guarantee convergence.\" % (1e-13 * C))\n",
    "        precision = 1e-13 * C\n",
    "    \n",
    "    epsilon = precision\n",
    "    step = 0\n",
    "    central_path = []\n",
    "    \n",
    "    while 2*m / t > epsilon:\n",
    "        if show_steps:\n",
    "            print(\"Step\", step)\n",
    "        step += 1\n",
    "        α, f_α = newton_svm(H, C, epsilon, alpha, beta, t=t, x_start=α, display=show_step_details)\n",
    "        t *= gamma\n",
    "        central_path.append(α)\n",
    "    \n",
    "    support_vectors_indices = np.where(α > 1e-6)[0]\n",
    "    support_vectors = X[support_vectors_indices]\n",
    "    support_vectors_weights = α[support_vectors_indices] * y[support_vectors_indices]\n",
    "    \n",
    "    print(\"Found %d support vectors.\" % len(support_vectors))\n",
    "    \n",
    "    results = dict()\n",
    "    results[\"α\"] = α\n",
    "    results[\"dual_solution\"] = α\n",
    "    results[\"support_vectors_indices\"] = support_vectors_indices\n",
    "    results[\"support_vectors_weights\"] = support_vectors_weights\n",
    "    results[\"support_vectors\"] = support_vectors\n",
    "    \n",
    "    def predict(X_test):\n",
    "        return kernel(X_test, support_vectors).dot(support_vectors_weights)\n",
    "    \n",
    "    def predict_labels(X_test):\n",
    "        return (predict(X_test) > 0.) * 2 - 1\n",
    "    \n",
    "    results[\"predict\"] = predict\n",
    "    results[\"predict_labels\"] = predict_labels\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Add more kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(mu1, sigma1, mu2, sigma2, N1, N2):\n",
    "    mu1 = np.array(mu1)\n",
    "    mu2 = np.array(mu2)\n",
    "    sigma1 = np.array(sigma1)\n",
    "    sigma2 = np.array(sigma2)\n",
    "    \n",
    "    mu = (mu1 + mu2) / 2.\n",
    "    mu1 = mu1 - mu\n",
    "    mu2 = mu2 - mu\n",
    "    X_train = np.concatenate([np.random.multivariate_normal(mu1, sigma1, N1), np.random.multivariate_normal(mu2, sigma2, N1)], axis=0)\n",
    "    y_train = np.concatenate([-np.ones(N1), np.ones(N1)], axis=0)\n",
    "    X_test = np.concatenate([np.random.multivariate_normal(mu1, sigma1, N2), np.random.multivariate_normal(mu2, sigma2, N2)], axis=0)\n",
    "    y_test = np.concatenate([-np.ones(N2), np.ones(N2)], axis=0)\n",
    "    return ((X_train, y_train), (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_dataset(dataset, letter=\"\", printing=True):\n",
    "    train_x, train_y = dataset[0]\n",
    "    plt.scatter(train_x[train_y == -1.][:, 0], train_x[train_y == -1.][:, 1], color='r')\n",
    "    plt.scatter(train_x[train_y == 1.][:, 0], train_x[train_y == 1.][:, 1], color='b')\n",
    "    plt.title(\"Dataset \" + letter)\n",
    "    xlim(np.min(dataset[0][0][:, 0]), np.max(dataset[0][0][:, 0]))\n",
    "    ylim(np.min(dataset[0][0][:, 1]), np.max(dataset[0][0][:, 1]))\n",
    "    \n",
    "    if printing:\n",
    "        plt.show()\n",
    "\n",
    "def make_meshgrid(X, n=100):\n",
    "    \"\"\"Create a mesh covering the inputs points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    n: grid size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x, y = X[:, 0], X[:, 1]\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, n),\n",
    "                         np.linspace(y_min, y_max, n))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf[\"predict\"](np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, [-1, 0, 1], **params)\n",
    "    ax.contour(xx, yy, Z, [-1, 0, 1], colors=['red', 'black', 'blue'])\n",
    "    return out\n",
    "\n",
    "def plot_results(dataset, clf, title=\"SVM\"):\n",
    "    if len(dataset[0][0][0]) != 2:\n",
    "        raise ValueError(\"Invalid dimension of dataset.\")\n",
    "    \n",
    "    X = dataset[0][0]\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    ax = plt.axes()\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm_r)\n",
    "\n",
    "    ax.scatter(clf[\"support_vectors\"][:, 0], clf[\"support_vectors\"][:, 1], s=100, linewidth=1, c='gray')\n",
    "    plot_dataset(dataset, printing=False)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    x_train, y_train = dataset[0]\n",
    "    x_test, y_test = dataset[1]\n",
    "    \n",
    "    train_labels = clf[\"predict_labels\"](x_train)\n",
    "    train_accuracy = np.mean(train_labels != y_train) * 100\n",
    "    test_labels = clf[\"predict_labels\"](x_test)\n",
    "    test_accuracy = np.mean(test_labels != y_test) * 100\n",
    "    print(\"Train Error: %.2f %%\\n Test Error: %.2f %%\" % (train_accuracy, test_accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array([[1., 0],\n",
    "           [0, 1]])\n",
    "y = array([1, -1])\n",
    "results = svm(X, y, 10)\n",
    "\n",
    "print(results[\"support_vectors\"].T.dot(results[\"support_vectors_weights\"]))\n",
    "print(results[\"α\"])\n",
    "\n",
    "sigma = 0.5\n",
    "cov = np.eye(2) * sigma\n",
    "dataset = generate([-1, -1], cov, [1, 1], cov, 100, 100)\n",
    "plot_dataset(dataset)\n",
    "results = svm(dataset[0][0], dataset[0][1], 1e3, show_steps=True, precision=1e-14)\n",
    "plot_results(dataset, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def linear_kernel(X1, X2):\n",
    "    return X1.dot(X2.T)\n",
    "\n",
    "def affine_kernel(X1, X2):\n",
    "    return X1.dot(X2.T) + 1\n",
    "\n",
    "def exponential_kernel_σ(X1, X2, σ=1):\n",
    "    return np.exp(-cdist(X1, X2) / (2*σ))\n",
    "\n",
    "def exponential_kernel(σ):\n",
    "    return lambda X1, X2: exponential_kernel_σ(X1, X2, σ)\n",
    "\n",
    "def polynomial_kernel_d(X1, X2, d=1):\n",
    "    return (affine_kernel(X1, X2)) ** d\n",
    "\n",
    "def polynomial_kernel(d):\n",
    "    return lambda X1, X2: polynomial_kernel_d(X1, X2, d)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    with open(filename, 'r') as src:\n",
    "        for line in src:\n",
    "            x1, x2, y = line.split()\n",
    "            x1 = float(x1)\n",
    "            x2 = float(x2)\n",
    "            y = float(y)\n",
    "            xs.append([x1, x2])\n",
    "            ys.append(y)\n",
    "    return np.array(xs), np.array(ys) * 2 - 1\n",
    "\n",
    "def import_dataset(letter):\n",
    "    train = \"%s/classification%s.train\" % (\"classification_data\", letter)\n",
    "    test = \"%s/classification%s.test\" % (\"classification_data\", letter )\n",
    "    \n",
    "    return read_file(train), read_file(test)\n",
    "\n",
    "def compute_results(letter, C=1):\n",
    "    dataset = import_dataset(letter)\n",
    "    train = dataset[0]\n",
    "    \n",
    "    for k in [affine_kernel, exponential_kernel(10), polynomial_kernel(7)]:\n",
    "        plot_results(dataset, svm(dataset[0][0], dataset[0][1], C, kernel=k), letter+\"with SVM\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results('A', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with LibSVM and LibLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "datasetA = import_dataset(\"A\")\n",
    "datasetA = ((datasetA[0][0] + 1, datasetA[0][1]), (datasetA[1][0] + 1, datasetA[1][1]))\n",
    "X, y = datasetA[0]\n",
    "\n",
    "C = 1\n",
    "%time results = svm(X, y, C)\n",
    "\n",
    "clf = LinearSVC(C=C, loss='hinge')\n",
    "# clf = SVC(C=C, kernel='linear')\n",
    "%time clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(datasetA, results, \"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
