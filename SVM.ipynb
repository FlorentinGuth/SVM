{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization Project\n",
    "## Support Vector Machines solvers\n",
    "\n",
    "Given $m$ data points $x_i \\in \\mathbb{R}^n$ with labels $y_i \\in \\{-1,1\\}$, write a function to solve the classification problem\n",
    "\n",
    "$$ \\begin{array}l\n",
    "\\mathrm{minimize} & \\frac12 {||w||}_2^2 + C \\mathbf{1}^Tz \\\\\n",
    "\\mathrm{subject\\ to} & y_i(w^Tx_i) \\geq 1 - z_i, \\quad \\forall i \\in \\{1,\\ldots,m\\} \\\\\n",
    "& z \\succcurlyeq 0\n",
    "\\end{array} $$\n",
    "\n",
    "in the variables $w \\in \\mathbb{R}^n$, $z \\in \\mathbb{R}^m$, and its dual (warning: this problem is a bit different from the one in exercise 1).\n",
    "\n",
    "Solving this problem trains a classifier vector $w$ such that, up to some errors\n",
    "\n",
    "$$ \\begin{array}l\n",
    "w^Tx_i > 0 & \\mathrm{when}\\ y_i = 1 \\\\\n",
    "w^Tx_i < 0 & \\mathrm{when}\\ y_i = -1.\n",
    "\\end{array} $$\n",
    "\n",
    "This classifier can then be used to classify new points $x$ as positives or negatives by simply computing the scalar product $w^Tx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the barrier method to solve both primal and dual problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, gradient_and_hessian, ɛ, x0, α=0.45, β=0.8):\n",
    "    \"\"\" Newton descent method.\n",
    "    @param f is the function to minimize\n",
    "    @param gradient_and_hessian is a function that returns the gradient and the hessian at x\n",
    "    @param ɛ is the required absolute precision\n",
    "    @param x0 is a strictly feasible point, i.e., f(x0) < +inf\n",
    "    @param α is a parameter for the backtracking line search\n",
    "    @param β is a parameter for the backtracking line search\n",
    "    @return an array of values\n",
    "    \"\"\"    \n",
    "    x = x0\n",
    "    values = [[copy(x), f(x)]]\n",
    "    while True:\n",
    "        # Direction computation.\n",
    "        gradient, hessian = gradient_and_hessian(x)\n",
    "        hessian_inv = inv(H)\n",
    "        dx = -hessian_inv.dot(gradient)\n",
    "        \n",
    "        # Stopping criterion.\n",
    "        λsquare = gradient.dot(-dx)\n",
    "        if λsquare/2 <= ɛ:\n",
    "            break\n",
    "        \n",
    "        # Backtracking line search.\n",
    "        t = 1.0\n",
    "        f0 = f(x)\n",
    "        Δ = -λsquare\n",
    "        while f(x + t*dx) > f0 + α*t*Δ:\n",
    "            t *= β\n",
    "        \n",
    "        # Update.\n",
    "        x += t*dx\n",
    "        values.append([copy(x), f(x)])\n",
    "    \n",
    "    return array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barrier_method(objective, objective_gh, m, constraints, constraints_gh, ɛ, x0, t0=1.0, μ=10.0, *newton_params):\n",
    "    \"\"\" Solve the SVM classifier problem.\n",
    "    @param objective is the function to minimize\n",
    "    @param objective_gh is a function which returns the gradient and the hessian of the objective\n",
    "    @param m is the number of (inequality) constraints\n",
    "    @param constraints is a function which returns the array of the f_i(x) where f_i(x) <= 0\n",
    "    @param constraints_gh is a function which returns the array of gradients and hessians of the -log(-fi(x))\n",
    "    @param x0 is a strictly feasible point, i.e., objective(x0) < +inf and constraints(x0) < 0\n",
    "    @param t0 is a barrier parameter\n",
    "    @param μ is a barrier parameter\n",
    "    @param *newton_params are the additional newton method parameters\n",
    "    @return an array of values\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    t = t0\n",
    "    values = []\n",
    "    \n",
    "    def f(x):\n",
    "        cons = constraints(x)\n",
    "        if any(cons) <= 0:\n",
    "            return float(\"inf\")\n",
    "        return objective(x) - sum(log(cons))/t\n",
    "    def gradient_and_hessian(x):\n",
    "        obj_g, obj_h = objective_gh(x)\n",
    "        cons_g, cons_h = constraints_gh(x)\n",
    "        return obj_g + cons_g/t, obj_h + cons_h/t\n",
    "    \n",
    "    while True:\n",
    "        x = newton(f, gradient_and_hessian, ɛ, x, *newton_params)[-1][0]\n",
    "        values.append([copy(v), objective(v)])\n",
    "        # Stopping criterion.\n",
    "        if m/t <= ɛ:\n",
    "            break\n",
    "        t *= μ\n",
    "    \n",
    "    return array(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the original problem, we need to solve the following problems for different values of $t$:\n",
    "\n",
    "$$ \\begin{array}c\n",
    "\\mathrm{minimize} & \\frac12 {||w||}_2^2 + C \\mathbf{1}^Tz - \\frac1t \\left[ \\sum_{i=1}^m \\log \\left( y_i(w^Tx_i) - 1 + z_i \\right) + \\sum_{i=1}^m \\log(z_i) \\right]\n",
    "\\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can reformulate the problem to:\n",
    "\n",
    "$$ \\begin{array}c\n",
    "\\mathrm{minimize} & \\frac12{||w||}_2^2 + C\\mathbf{1}^Tz - \\frac1t \\sum_{i=1}^{2m} \\log(b_i - a_i^Tv)\n",
    "\\end{array} \\\\ \\\\\n",
    "\\mathrm{with}\\quad v = (w,z) \\in \\mathbb{R}^{n+m}\\quad b_i = \\left\\{ \\begin{array}l -1 & \\text{if}\\ i \\leq m \\\\ 0 & \\text{if}\\ i > m \\end{array}\\right. \\quad a_i = \\left\\{\\begin{array}l -(y_ix_i,e_i) & \\text{if}\\ i \\leq m \\\\ (0,e_{i-m}) & \\text{if}\\ i > m \\end{array}\\right.\\quad (e_i)_{1\\leq i\\leq m} \\text{ is the canonical base of } \\mathbb{R}^m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(w,z) = \\frac12 {||w||}_2^2 + C \\mathbf{1}^Tz - \\frac1t \\sum_{i=1}^{2m} \\log(b_i - a_i^Tv) $$\n",
    "\n",
    "$$ \\nabla_v f(v) = (w,C\\mathbf{1}) + \\frac1t\\sum_{i=1}^{2m} \\frac{a_i}{b_i - a_i^Tv} $$\n",
    "\n",
    "$$ H_f(v) = \\left( \\begin{array}c\n",
    "I_n & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "\\end{array} \\right) + \\frac1t \\sum_{i=1}^{2m} \\frac{a_i a_i^T}{\\left( b_i - a_i^Tv \\right)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_barrier(x, y, c):\n",
    "    \"\"\" Solve the SVM classifier problem.\n",
    "    @param x is an array of shape (m,n): the m points in dimension n\n",
    "    @param y is an array of shape m and of values in {-1,1}: the labels\n",
    "    @param c is the margin parameter\n",
    "    @return (w,z) where w is the classifier vector (array of shape n) and z the margin vector (array of shape m)\n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    # w = 0 and z_i = 2 is a strictly feasible point.\n",
    "    v0 = concatenate((zeros(n), 2*ones(m)))\n",
    "    a = concatenate((\n",
    "            concatenate(((y*x.T).T, eye(m)), axis=1), \n",
    "            concatenate((zeros((m, n)), eye(m)), axis=1)), \n",
    "            axis=0)\n",
    "    b = concatenate(-ones(m), zeros(m))\n",
    "    \n",
    "    scalars = array([a[i].reshape(n,1).dot(a[i].reshape(1,n)) for i in range(m)])\n",
    "    def objective(v):\n",
    "        w, z = x[:n], x[n:]\n",
    "        return sum(w**2)/2 + c*sum(z)\n",
    "    def objective_gh(v):\n",
    "        w, z = x[:n], x[n:]\n",
    "        gradient = concatenate(w, c*ones(m))\n",
    "        hessian = concatenate((\n",
    "            concatenate((eye(n), zeros((m, m))), axis=1),\n",
    "            zeros((m, n+m)),\n",
    "        ), axis=0)\n",
    "        return gradient, hessian\n",
    "    def constraints(v):\n",
    "        return a.dot(v) - b\n",
    "    def constraints_gh(v):\n",
    "        gradient = sum((a.T / div).T, axis=0)/t\n",
    "        hessian = sum((scalars.T / (div**2)).T, axis=0)/t\n",
    "        return gradient, hessian\n",
    "    \n",
    "    ɛ = 1e-10\n",
    "    values = barrier_method(objective, objective_gh, m, constraints, constraints_gh, ɛ, v0)\n",
    "    v = values[-1][0]\n",
    "    return v[:n], v[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test your code on random clouds of points (e.g. generate two classes of data points by picking two bivariate Gaussian samples with different moments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_points(n):\n",
    "    \"\"\" Generates a dataset composed of two bivariate gaussian samples with different means.\n",
    "    @param n is the number of points in each class\n",
    "    @return \"\"\"\n",
    "    σ = \n",
    "    μ1 = \n",
    "    μ2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try various values of $C > 0$ and measure out-of-sample performance (i.e. classification errors on points the algorithm has not seen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot duality gap versus iteration number as well as a separation example in 2D (you may add a constant coefficient to the data points $x$ to allow classifiers that do not go through the origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Use CVX (MATLAB or OCTAVE) or CVXOPT (python), as well as LIBSVM and/or LIBLINEAR to check your results and compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Use the coordinate descent method to solve the dual. Plot duality gap versus iteration number and compare performance with the barrier method for various problem sizes (vary the number of samples and record to total CPU time required by each code to reduce the gap by a factor ${10}^{-3}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Use the logarithmic barrier code you wrote in HW1 to solve a small random instance of the primal problem using the ACCPM algorithm. Plot an upper bound on the distance to optimality in semilog scale and try various constraint dropping strategies. Compare convergence with the two other methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
